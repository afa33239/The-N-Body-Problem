Phase 6 — Profiling & Optimization (In Progress)

Note
These logs were reviewed and lightly reorganized after a break from the project.
No results, measurements, or conclusions were changed.
The purpose of the rewrite is to improve readability and structure only.

Goal
The goal of Phase 6 is to identify performance hotspots in the N-body simulation
before attempting further optimization.
All optimization decisions in this phase are based on measured data rather than assumptions.

General Approach
Profiling is done using Python’s built-in cProfile.
The full simulation pipeline is profiled, including solver, integrator, and diagnostics.
Results are sorted by cumulative runtime to identify the dominant cost areas.
Both Direct and Barnes–Hut solvers are profiled separately.
Real runtime measurements (without profiling) are used to confirm improvements,
since profiling overhead can distort results for recursive code.




--- Direct Solver — Initial Profiling


Setup
Integrator: Leapfrog (Velocity Verlet)
Solver: Direct O(N^2)
N = 300
Steps = 300
dt = 2e-3
Softening = 1e-3

Results
Total runtime was approximately 95 seconds under profiling.
Most of the runtime is spent inside physics.compute_accelerations.
This matches the expected O(N^2) scaling of the Direct solver.
Leapfrog integration adds additional cost due to repeated force evaluations
and synchronization.
Diagnostics such as energy, momentum, and center-of-mass tracking contribute
noticeable but smaller overhead.
State copying and object creation appear in the profile but are not the main hotspots.

Interpretation
The Direct solver behaves exactly as expected from theory.
Pairwise force computation dominates runtime and defines the performance limit.
This provides a correct baseline for comparison in Phase 6.


--- Barnes–Hut Solver — Initial Profiling


Setup
Integrator: Leapfrog (Velocity Verlet)
Solver: Barnes–Hut Octree (theta = 0.7)
N = 1000
Steps = 300
dt = 2e-3
Softening = 1e-3

Results
Total runtime was approximately 1038 seconds under profiling.
Runtime is dominated by recursive tree traversal during force evaluation.
OctreeNode.compute_accelerations accounts for the majority of cumulative runtime.
Tree construction steps such as insert, subdivide, and mass or center-of-mass
updates contribute noticeable but smaller overhead.
Function call counts are extremely high due to recursion and repeated node visits.
Leapfrog integration increases traversal cost because accelerations are evaluated
multiple times per timestep.
Diagnostics are expensive but not the dominant cost.

Interpretation
Although Barnes–Hut has better asymptotic scaling, Python-level overhead from
recursion, object handling, and attribute access significantly reduces its
practical performance advantage at this stage.


--- Profiling With Diagnostics Disabled


Direct Solver
Total runtime was approximately 72 seconds.
The main hotspot remains physics.compute_accelerations.
This confirms that Direct solver performance is dominated by the O(N^2) force loop.
Integrator overhead is present but secondary.

Barnes–Hut Solver
Total runtime was approximately 760 seconds.
Function call counts remain extremely high due to recursive traversal.
OctreeNode.compute_accelerations dominates cumulative runtime.
Tree construction costs are visible but not the primary hotspots.
Profiling overhead disproportionately affects Barnes–Hut because of recursion.

Interpretation
Disabling diagnostics confirms that:
Direct solver cost comes from pairwise force computation.
Barnes–Hut cost comes from Python recursion and object-level traversal.
The algorithmic advantage of Barnes–Hut is masked by Python overhead under profiling.




--- Optimization Attempt 1 — Barnes–Hut Tree Traversal


Change
Small local optimizations were made inside OctreeNode.compute_accelerations:
Frequently accessed attributes were cached into local variables.
Duplicate distance calculations were removed.
Softening squared was precomputed.
Early returns were added in approximation cases.

Reason
Profiling showed that OctreeNode.compute_accelerations is the dominant hotspot.
The goal was to reduce Python overhead per call without changing the algorithm.

Expected Effect
A small reduction in runtime due to fewer attribute lookups and repeated math
operations in a heavily used function.

Verification
Barnes–Hut was run with N = 10 and diagnostics enabled after the change.
The simulation completed normally and diagnostics behaved as expected.


--- Profiling After Optimization Attempt 1


Results
Re-profiled Barnes–Hut with N = 1000 and diagnostics enabled.
Time spent per call inside compute_accelerations decreased.
Overall profiling runtime increased.
Function call count increased significantly.

Interpretation
The optimization reduced work per call but did not reduce the number of
recursive calls.
Profiler overhead increased because of the higher call count, masking real
runtime improvements.


--- Real Runtime Measurements (No Profiling)


Setup
Identical configuration to profiling runs, but without cProfile enabled (time_barnes.py).

Results
Original Barnes–Hut implementation runtime was approximately 349.5 seconds.
Optimized implementation runtime was approximately 233.1 seconds.
This corresponds to an improvement of roughly 33 percent.

Interpretation
Although profiling results were unclear, real runtime measurements show a
substantial speedup.
Reducing per-call overhead in compute_accelerations is effective.

Conclusion
The optimization should be kept.
Further improvements should focus on reducing recursion depth and total
function call count.


--- Additional Verification (profile_phase6.py)


Barnes–Hut was run with N = 50 and diagnostics enabled after the changes.
The simulation completed normally with finite drift values and no crashes.

Testing Notes
Timing tests were moved from profile_phase6.py to time_barnes.py for the rest of Phase 6.


--- Timing Recheck


Setup
N = 1000
Steps = 300
Theta = 0.7
Diagnostics disabled

Result
Measured runtime was approximately 351.9 seconds.

Note
This runtime is similar to the earlier baseline of about 349.5 seconds.
The difference may be due to configuration differences or missing cache effects.
Further checks are required to confirm identical conditions.


--- Clarification Profiling Run (profile_phase6.py)


Purpose
This profiling run was performed to remove confusion about the main Barnes–Hut
performance hotspots before continuing optimization.

Results
The majority of total runtime is spent inside OctreeNode.compute_accelerations.
Tree construction accounts for a much smaller portion of the total runtime.

Interpretation
Caching the octree alone would provide limited benefit.
The primary performance issue is repeated tree traversal for force evaluation.

Conclusion
Phase 6 optimization should focus on reducing the number of Barnes–Hut force
evaluations per timestep, rather than only optimizing tree construction.


--- Clarification Profiling Run 2 (profile_phase6.py)


Setup
Same configuration.

Purpose
Second profiling run was performed with the last optimised implementation of compute_accelerations. 
Previous run used old implementation of compute_accelerations.

Results
Same results achieved.






--- Optimization Attempt 2 — Eliminating Redundant Barnes–Hut Force Evaluations


Change
Modified Leapfrog integration to cache Barnes–Hut accelerations and reuse them
between the integration step and synchronization.
Accelerations computed during the kick stage are stored on the system state and
reused during synchronize instead of recomputing forces.


Reason
Profiling showed that Barnes–Hut force evaluation was being executed multiple
times per timestep.
This resulted in repeated octree traversal at identical positions.
The goal was to reduce the number of calls to octree.compute_accelerations
without changing the Barnes–Hut algorithm.


Expected Effect
A significant reduction in total runtime by eliminating one full Barnes–Hut
tree traversal per timestep.


Verification
Barnes–Hut was run with small N and diagnostics enabled after the change.
The simulation completed normally and diagnostics behaved as expected.


Results (profile_phase6.py)
Re-profiled Barnes–Hut with the same configuration as previous profiling runs.
Total profiled runtime decreased substantially.
The number of function calls dropped significantly.
octree.compute_accelerations remains the dominant hotspot but is called fewer
times per timestep.


Interpretation
Caching accelerations successfully removed redundant Barnes–Hut force
evaluations.
The remaining runtime cost now corresponds to the minimum required tree
traversal per timestep.


Conclusion
This optimization is effective and should be kept.
Further Phase 6 work should focus on reducing the cost of the remaining
Barnes–Hut traversal or transitioning to lower-level optimizations.




--- Phase 6 Final Timing Comparison — Direct vs Barnes–Hut


Setup
Integrator: Leapfrog (Velocity Verlet)
Solvers: Direct O(N^2) and Barnes–Hut Octree
Theta: 0.7
N: 1000
Steps: 300
dt: 2e-3
Softening: 1e-3
Diagnostics: disabled
Each solver was run three times using identical initial conditions.


Results
Barnes–Hut runtimes:
~117.5 s
~121.5 s
~123.4 s

Direct solver runtimes:
~180.4 s
~184.7 s
~181.2 s


Comparison
Barnes–Hut consistently outperforms the Direct solver for this configuration.
Using the best runtimes from each solver, Barnes–Hut is approximately 1.5× faster
than Direct at N = 1000.


Interpretation
After eliminating redundant Barnes–Hut force evaluations and optimizing tree
traversal, the Barnes–Hut solver achieves a clear performance advantage over
the Direct solver at large N.
Remaining Barnes–Hut cost is dominated by unavoidable tree traversal and
Python-level recursion overhead.


Conclusion
Phase 6 optimization goals have been met.
Barnes–Hut now demonstrates both correct behavior and practical performance
benefits over the Direct solver for sufficiently large systems.


Next Steps
Further performance improvements are possible but would require more invasive
changes (e.g. iterative traversal, NumPy/Numba acceleration).
These are deferred to a future optimization phase.




-- Phase 6 End --
